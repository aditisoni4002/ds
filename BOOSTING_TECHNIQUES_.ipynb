{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BOOSTING TECHNIQUES"
      ],
      "metadata": {
        "id": "FFGcyQe9ac6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Answer:\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners (models that perform only slightly better than random guessing) to create a strong learner. The key principle is sequential learning where each new weak learner focuses on correcting the mistakes made by previous ones.\n",
        "\n",
        "How it improves weak learners:\n",
        "1. Sequential Correction: Misclassified samples from the previous model get higher weights, forcing new models to focus on these difficult cases\n",
        "2. Weighted Voting: Final predictions are made by combining all weak learners using weighted votes (better performers get higher weights)\n",
        "3. Iterative Refinement: Each iteration builds upon the previous model's errors, progressively reducing bias\n",
        "4. Cumulative Strength: The ensemble leverages the collective decision-making of all weak learners to make robust predictions\n",
        "\n",
        "---\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Answer:\n",
        "AdaBoost (Adaptive Boosting):\n",
        "- Trains weak learners sequentially\n",
        "- Adjusts sample weights based on misclassification errors\n",
        "- Higher weights given to misclassified samples in the next iteration\n",
        "- Final prediction: Weighted sum of all weak learner predictions\n",
        "- Focuses on adjusting data distribution to focus on hard examples\n",
        "- Loss function: Exponential loss\n",
        "- Typically uses decision trees (stumps) as weak learners\n",
        "\n",
        "Gradient Boosting:\n",
        "- Also trains weak learners sequentially\n",
        "- Fits new learners to the negative gradient of the loss function (residuals)\n",
        "- Each new tree fits the errors/residuals of the previous ensemble\n",
        "- Final prediction: Sequential addition of weak learner outputs\n",
        "- Focuses on reducing loss through gradient descent optimization\n",
        "- Loss function: Can be customized (squared error, log loss, etc.)\n",
        "- More flexible and generally outperforms AdaBoost on many tasks\n",
        "- Examples: XGBoost, LightGBM, CatBoost\n",
        "\n",
        "---\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer:\n",
        "XGBoost incorporates regularization to prevent overfitting and improve generalization. Key regularization techniques:\n",
        "\n",
        "1. L1 Regularization (Lasso):\n",
        "- Penalizes the sum of absolute values of tree weights\n",
        "- Can force some weights to exactly zero, performing feature selection\n",
        "- Parameter: alpha (default=0)\n",
        "\n",
        "2. L2 Regularization (Ridge):\n",
        "- Penalizes the sum of squared tree weights\n",
        "- Shrinks weights of less important features\n",
        "- Parameter: lambda (default=1)\n",
        "\n",
        "3. Tree Complexity Regularization:\n",
        "- gamma: Minimum loss reduction to create a split (higher = more conservative)\n",
        "- max_depth: Limits tree depth, reducing model complexity\n",
        "- min_child_weight: Minimum sum of instance weight needed for a child node\n",
        "\n",
        "4. Column Subsampling:\n",
        "- subsample: Fraction of samples used for fitting each tree\n",
        "- colsample_bytree: Fraction of features used for fitting each tree\n",
        "\n",
        "Benefits:\n",
        "- Reduces overfitting by constraining model complexity\n",
        "- Improves generalization to unseen data\n",
        "- Makes the model more interpretable\n",
        "- Prevents individual trees from becoming too deep and specialized\n",
        "\n",
        "---\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer:\n",
        "CatBoost (Categorical Boosting) is specifically optimized for categorical features:\n",
        "\n",
        "1. Native Categorical Support:\n",
        "- Automatically handles categorical features without one-hot encoding\n",
        "- Stores categorical features in their original form, reducing memory usage\n",
        "- No need for manual preprocessing of categorical variables\n",
        "\n",
        "2. Ordered Target Encoding:\n",
        "- Converts categorical features to numerical using target statistics\n",
        "- Uses ordered boosting approach to reduce overfitting\n",
        "- Each sample is encoded using statistics from previous samples only\n",
        "\n",
        "3. Efficient Memory Usage:\n",
        "- Avoids the curse of dimensionality from one-hot encoding\n",
        "- Reduces feature space significantly with many categorical variables\n",
        "- Faster training and prediction times\n",
        "\n",
        "4. Reduced Overfitting:\n",
        "- Special treatment of categorical features prevents overfitting\n",
        "- Target encoding is applied during training to avoid data leakage\n",
        "- Ordered principle ensures target statistics don't leak information\n",
        "\n",
        "5. Better Performance:\n",
        "- Automatic feature interactions for categorical variables\n",
        "- Out-of-the-box handling without hyperparameter tuning for encoding\n",
        "- Superior performance on datasets with high-cardinality categorical features\n",
        "\n",
        "6. Handling Unknown Categories:\n",
        "- Robust handling of categories not seen during training\n",
        "- Better generalization on new data\n",
        "\n",
        "---\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Answer:\n",
        "Boosting is preferred over bagging in several real-world scenarios:\n",
        "\n",
        "1. Medical Diagnosis & Healthcare:\n",
        "- Predicting disease risk with high accuracy required\n",
        "- False negatives can have serious consequences\n",
        "- Boosting's focus on hard cases improves minority class detection\n",
        "\n",
        "2. Credit Risk & Fraud Detection:\n",
        "- Fraud cases are rare (class imbalance)\n",
        "- Boosting handles imbalanced data better by focusing on minority class\n",
        "- False positives and false negatives have different costs\n",
        "\n",
        "3. Search Ranking & Recommendation Systems:\n",
        "- LambdaMART (gradient boosting variant) powers search ranking\n",
        "- Sensitive to ranking of top results\n",
        "- Boosting learns to prioritize important items\n",
        "\n",
        "4. Computer Vision & Object Detection:\n",
        "- Cascade classifiers (boosted weak classifiers) for face detection\n",
        "- Sequential refinement improves detection accuracy\n",
        "\n",
        "5. Spam Detection:\n",
        "- Class imbalance (spam is minority)\n",
        "- Boosting better focuses on distinguishing spam patterns\n",
        "\n",
        "6. Demand Forecasting:\n",
        "- Accurate predictions crucial for inventory management\n",
        "- Boosting reduces bias better than bagging\n",
        "\n",
        "7. Anomaly Detection:\n",
        "- Rare anomalies need special focus\n",
        "- Boosting's weighted approach highlights anomalous patterns\n",
        "\n",
        "8. Stock Price Prediction & Time Series:\n",
        "- Sequential nature of boosting suits temporal data\n",
        "- Focuses on correcting previous prediction errors\n",
        "\n",
        "Why Boosting > Bagging:\n",
        "- Better for high-bias, low-variance problems\n",
        "- Superior when interpretability and feature importance are needed\n",
        "- More effective with small to medium-sized datasets\n",
        "- Better sequential learning for dependent data\n",
        "- Handles class imbalance more naturally\n",
        "- More aggressive bias reduction strategy"
      ],
      "metadata": {
        "id": "nPG50oo_ZiOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "t0Izx93qa-26"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101d16a3",
        "outputId": "3a5a8ecd-dca1-4f58-9d02-f649e38ba760"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "bc_data = load_breast_cancer()\n",
        "X = bc_data.data\n",
        "y = bc_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the AdaBoost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy on Breast Cancer dataset: {accuracy:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer dataset: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "SJLfFfJFdQt9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d1e43c3",
        "outputId": "c4989460-3eb5-4f5d-a80b-ccfb60e8a238"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X_housing = california_housing.data\n",
        "y_housing = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(X_housing, y_housing, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train_housing, y_train_housing)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_housing = gbr.predict(X_test_housing)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test_housing, y_pred_housing)\n",
        "print(f\"Gradient Boosting Regressor R-squared score on California Housing dataset: {r2:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score on California Housing dataset: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "88gajNmZdhad"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ba2c836",
        "outputId": "f19bddbb-12c2-4bff-9881-87226d12c6f6"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "bc_data = load_breast_cancer()\n",
        "X = bc_data.data\n",
        "y = bc_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "# Use 'objective='binary:logistic' for binary classification and 'eval_metric='logloss'' for evaluation\n",
        "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Make predictions on the test set with the best estimator\n",
        "best_xgb_clf = grid_search.best_estimator_\n",
        "y_pred_xgb = best_xgb_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Classifier Test Accuracy (with best learning_rate): {accuracy_xgb:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "Best parameters: {'learning_rate': 0.1}\n",
            "Best cross-validation accuracy: 0.9626\n",
            "XGBoost Classifier Test Accuracy (with best learning_rate): 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:17:46] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "yR4aOsEydwRS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "9e519400",
        "outputId": "f15e39f6-16f5-4ce7-b476-35bf8e463906"
      },
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer # Assuming breast cancer data for classification\n",
        "\n",
        "# Load the Breast Cancer dataset (if not already loaded)\n",
        "# This uses the X and y defined in previous cells for consistency.\n",
        "bc_data = load_breast_cancer()\n",
        "X = bc_data.data\n",
        "y = bc_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "# verbose=0 to suppress training output, random_state for reproducibility\n",
        "cat_clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='Logloss', verbose=0, random_state=42)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_cat = cat_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
        "print(f\"CatBoost Classifier Accuracy: {accuracy_cat:.4f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_cat)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=bc_data.target_names, yticklabels=bc_data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('CatBoost Classifier Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "CatBoost Classifier Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "9fd16xIveH8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ea9ba93"
      },
      "source": [
        "## Data Preprocessing and Handling\n",
        "\n",
        "### Subtask:\n",
        "Describe the steps for data preprocessing, including handling missing values (e.g., imputation techniques like mean, median, mode, or more advanced methods), and managing categorical features (e.g., one-hot encoding, target encoding, or using algorithms like CatBoost that handle them natively). Emphasize strategies for dealing with imbalanced datasets (e.g., oversampling, undersampling, SMOTE, or class weighting).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37387dd5"
      },
      "source": [
        "### Data Preprocessing and Handling Strategies\n",
        "\n",
        "In the context of predicting loan default, where the dataset is imbalanced and contains missing values along with both numeric and categorical features, a robust data preprocessing pipeline is crucial. Here's a breakdown of the strategies:\n",
        "\n",
        "#### 1. Handling Missing Values\n",
        "Missing values are common in real-world datasets and must be addressed to prevent model errors and biases. The choice of imputation method depends on the nature of the data and the extent of missingness:\n",
        "\n",
        "*   **Simple Imputation Techniques:**\n",
        "    *   **Mean/Median Imputation (for numerical features):** For numerical features, replacing missing values with the mean or median of the column is a straightforward approach. Median is often preferred when the data is skewed or contains outliers, as it is less sensitive to extreme values. For example, `df['numeric_col'].fillna(df['numeric_col'].median(), inplace=True)`.\n",
        "    *   **Mode Imputation (for categorical features):** For categorical features, replacing missing values with the mode (most frequent category) is a common strategy. For example, `df['categorical_col'].fillna(df['categorical_col'].mode()[0], inplace=True)`.\n",
        "    *   **Constant Value Imputation:** Replacing missing values with a constant (e.g., 0, -999, or 'Unknown') can be used, especially if the missingness itself is informative. Care should be taken not to introduce artificial patterns.\n",
        "\n",
        "*   **Advanced Imputation Techniques:**\n",
        "    *   **K-Nearest Neighbors (KNN) Imputation:** This method imputes missing values by finding the `k` nearest neighbors to the observation with missing data and using the values from these neighbors to estimate the missing value (e.g., mean for numerical, mode for categorical). This can capture more complex relationships but is computationally more expensive.\n",
        "    *   **Predictive Model Imputation (e.g., Iterative Imputer):** Missing values are treated as a target variable, and other features are used to predict them using a machine learning model. This is more sophisticated but can be complex and time-consuming.\n",
        "    *   **Deletion:** If the percentage of missing values is very small (e.g., less than 5%) and randomly distributed, rows or columns with missing data can sometimes be deleted, but this risks losing valuable information.\n",
        "\n",
        "#### 2. Managing Categorical Features\n",
        "Categorical features need to be converted into a numerical format that machine learning models can understand. The choice of encoding depends on whether the categories have an inherent order (ordinal) or not (nominal).\n",
        "\n",
        "*   **For Nominal Variables (no inherent order):**\n",
        "    *   **One-Hot Encoding:** Creates a new binary column for each category. If a row belongs to a category, the corresponding column gets a '1', and others get '0'. This is suitable for models that cannot handle categorical data directly and avoids implying ordinal relationships. However, it can lead to high dimensionality (`curse of dimensionality`) for high-cardinality features.\n",
        "\n",
        "*   **For Ordinal Variables (inherent order):**\n",
        "    *   **Ordinal Encoding:** Assigns an integer to each category based on its order (e.g., 'Low'=1, 'Medium'=2, 'High'=3). This preserves the ordinal relationship but should only be used when such an order truly exists.\n",
        "\n",
        "*   **Advanced/Algorithm-Specific Handling:**\n",
        "    *   **Target Encoding (Mean Encoding):** Replaces a category with the mean of the target variable for that category. This can be very effective but is prone to overfitting, especially with high-cardinality features. Cross-validation or adding noise can mitigate this risk.\n",
        "    *   **Native Categorical Feature Handling (e.g., CatBoost):** Boosting algorithms like CatBoost are designed to handle categorical features natively. CatBoost uses a unique ordered target encoding scheme and permutation-based feature importance to prevent target leakage and overfitting. It automatically converts categorical features into numerical values during training, eliminating the need for manual encoding and often yielding better performance, especially with many categorical variables.\n",
        "\n",
        "#### 3. Addressing Imbalanced Datasets\n",
        "Loan default datasets are typically imbalanced, meaning the number of non-defaulting loans significantly outweighs defaulting ones. If not addressed, the model might become biased towards the majority class, performing poorly on the minority (defaulting) class, which is often the class of interest. Strategies include:\n",
        "\n",
        "*   **Resampling Techniques:**\n",
        "    *   **Oversampling Minority Class (e.g., SMOTE):** Increases the number of instances in the minority class. SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic samples for the minority class by interpolating between existing minority class samples rather than simply duplicating them. Other methods include Random Oversampling (simply duplicating minority samples).\n",
        "    *   **Undersampling Majority Class (e.g., Random Undersampling):** Reduces the number of instances in the majority class. Random Undersampling randomly removes samples from the majority class. More advanced techniques like Tomek Links or NearMiss selectively remove majority samples that are close to minority samples, aiming to improve class separation.\n",
        "    *   **Combined Approaches:** Using both oversampling and undersampling (e.g., SMOTEENN or SMOTETomek) can often yield better results than either technique alone.\n",
        "\n",
        "*   **Algorithm-Level Approaches:**\n",
        "    *   **Class Weighting:** Most classification algorithms (including boosting methods) allow assigning different weights to classes during training. By giving a higher weight to the minority class, the model is penalized more for misclassifying minority samples, forcing it to pay more attention to them. This is often achieved through parameters like `scale_pos_weight` in XGBoost or `class_weights` in scikit-learn models. This method avoids altering the dataset distribution directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e5913b"
      },
      "source": [
        "## Boosting Algorithm Choice\n",
        "\n",
        "### Subtask:\n",
        "Discuss the pros and cons of AdaBoost, XGBoost, and CatBoost in the context of loan default prediction, and justify the selection of a specific boosting algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eefb7a7"
      },
      "source": [
        "### Subtask: Boosting Algorithm Choice\n",
        "\n",
        "#### Analysis of Boosting Algorithms for Loan Default Prediction:\n",
        "\n",
        "In a FinTech scenario predicting loan default, we face several challenges:\n",
        "1.  **Imbalanced Datasets:** Loan defaults are typically a minority class, meaning there are far fewer instances of default than non-default. This imbalance can lead models to be biased towards the majority class, performing poorly on identifying actual defaults.\n",
        "2.  **Mixed Feature Types:** Customer demographics (e.g., age, income, credit score) are numeric, while transaction behavior or other attributes (e.g., loan type, marital status) can be categorical.\n",
        "3.  **Missing Values:** Real-world datasets often have missing information for various reasons.\n",
        "4.  **Overall Performance and Flexibility:** The chosen model needs to be robust, accurate, and adaptable to various data characteristics.\n",
        "\n",
        "Let's analyze AdaBoost, XGBoost, and CatBoost against these criteria:\n",
        "\n",
        "#### 1. AdaBoost (Adaptive Boosting):\n",
        "*   **Handling of Imbalanced Datasets:** AdaBoost inherently addresses imbalanced data to some extent by increasing the weight of misclassified samples in subsequent iterations. If the minority class samples are misclassified, their weights will be boosted, forcing subsequent learners to focus on them. However, it can be sensitive to noisy data and outliers, which might be common in financial datasets, as these 'hard' samples might be given undue importance.\n",
        "*   **Ability to Process Mixed Feature Types:** AdaBoost itself doesn't have native support for categorical features. It typically requires manual preprocessing like one-hot encoding, which can lead to high-dimensional sparse data, especially with high-cardinality categorical features.\n",
        "*   **Robustness to Missing Values:** AdaBoost does not have built-in handling for missing values. These must be imputed or handled during preprocessing.\n",
        "*   **Overall Performance and Flexibility:** Generally simpler and faster than gradient boosting methods. It often uses simple decision stumps (depth 1) as base learners, making it less prone to overfitting than complex trees. However, its performance might be surpassed by more advanced boosting algorithms for complex, high-dimensional data.\n",
        "\n",
        "#### 2. XGBoost (Extreme Gradient Boosting):\n",
        "*   **Handling of Imbalanced Datasets:** XGBoost can handle class imbalance using parameters like `scale_pos_weight` (to balance positive and negative weights) or by adjusting the `min_child_weight` parameter. It also benefits from the flexibility of its loss function, allowing for custom loss functions that can better penalize errors on the minority class.\n",
        "*   **Ability to Process Mixed Feature Types:** XGBoost primarily works with numerical data. Categorical features generally require one-hot encoding or other numerical conversions (e.g., label encoding, target encoding) before feeding into XGBoost. One-hot encoding can suffer from the 'curse of dimensionality' with many high-cardinality categorical features.\n",
        "*   **Robustness to Missing Values:** XGBoost has a built-in mechanism to handle missing values. It learns the best direction to send samples with missing values by assigning them to the default direction in each node split based on the split gain, without needing explicit imputation.\n",
        "*   **Overall Performance and Flexibility:** Highly optimized and known for its speed and performance. It includes strong regularization techniques (L1, L2, tree complexity) to prevent overfitting, which is crucial in high-stakes applications like loan default prediction. It's highly flexible with various objective functions and evaluation metrics.\n",
        "\n",
        "#### 3. CatBoost (Categorical Boosting):\n",
        "*   **Handling of Imbalanced Datasets:** CatBoost, like XGBoost, can handle class imbalance through techniques such as `scale_pos_weight` or by adjusting sampling strategies. Its advanced target encoding strategy also helps in reducing bias which can be beneficial for imbalanced classes.\n",
        "*   **Ability to Process Mixed Feature Types:** This is CatBoost's standout feature. It has native and efficient handling of categorical features, eliminating the need for explicit one-hot encoding. It uses a sophisticated \"ordered target encoding\" method that converts categorical features to numerical values on the fly during training, preventing target leakage and making it highly effective for datasets with many categorical variables.\n",
        "*   **Robustness to Missing Values:** CatBoost can handle missing values automatically. It treats `NaN` as a distinct category or value, similar to how it handles categorical features, without requiring imputation.\n",
        "*   **Overall Performance and Flexibility:** CatBoost is designed for high accuracy and robustness. Its ordered boosting algorithm reduces prediction shift and overfitting, which is a common issue in gradient boosting. It's often slower to train than XGBoost due to its sophisticated categorical feature handling but can yield superior performance on datasets with many categorical features.\n",
        "\n",
        "#### Comparison and Selection Justification:\n",
        "\n",
        "For a FinTech company predicting loan default with a dataset that is **imbalanced, contains missing values, and has both numeric and categorical features**, CatBoost emerges as the most suitable choice.\n",
        "\n",
        "*   **Imbalanced Datasets:** While all three can address imbalance, CatBoost's advanced encoding and boosting scheme, combined with standard techniques, provide robust handling.\n",
        "*   **Mixed Feature Types:** CatBoost's **native handling of categorical features** is a significant advantage. In a financial context, features like 'loan type', 'industry of employment', or 'marital status' can be categorical and might have high cardinality. CatBoost's ordered target encoding prevents target leakage and the explosion of features that would occur with one-hot encoding in XGBoost or AdaBoost, simplifying preprocessing and often leading to better performance.\n",
        "*   **Missing Values:** Like XGBoost, CatBoost has **built-in support for missing values**, eliminating the need for complex imputation strategies that could introduce bias or additional noise.\n",
        "*   **Overall Performance and Flexibility:** CatBoost's ordered boosting and regularization strategies are designed to reduce overfitting and produce accurate models. Given the high stakes of loan default prediction, a model that is robust and minimizes the risk of overfitting is highly desirable.\n",
        "\n",
        "**Justification for CatBoost:**\n",
        "CatBoost is preferred over AdaBoost and XGBoost primarily due to its **native and efficient handling of categorical features** and its **robustness against overfitting** through ordered boosting and special target encoding. Manual preprocessing for categorical features (one-hot encoding) in AdaBoost and XGBoost can be cumbersome, computationally expensive, and lead to less optimal models in the presence of many high-cardinality categorical variables. For loan default prediction where accuracy and interpretability (or at least robustness) are paramount, and the data often contains a mix of feature types with missing values, CatBoost provides a streamlined and powerful solution, reducing the need for extensive feature engineering and minimizing the risk of data leakage from categorical encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889ad4da"
      },
      "source": [
        "## Hyperparameter Tuning Strategy\n",
        "\n",
        "### Subtask:\n",
        "Explain a suitable hyperparameter tuning strategy for the chosen CatBoost model, including important hyperparameters and cross-validation considerations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfbe47b1"
      },
      "source": [
        "### 1. Explanation of Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model that yields the best performance on unseen data. Unlike model parameters, which are learned from the data (e.g., weights in a neural network or splits in a decision tree), hyperparameters are external configuration variables set *before* the training process begins.\n",
        "\n",
        "**Why it's crucial for boosting algorithms like CatBoost:**\n",
        "\n",
        "Boosting algorithms are powerful but complex. Their performance is highly sensitive to the chosen hyperparameters. Improperly tuned hyperparameters can lead to:\n",
        "\n",
        "*   **Underfitting:** If hyperparameters are too restrictive (e.g., very low learning rate, shallow trees), the model might not capture the underlying patterns in the data, leading to high bias and poor performance.\n",
        "*   **Overfitting:** If hyperparameters are too permissive (e.g., very high learning rate, deep trees, many iterations), the model might learn the noise in the training data, leading to low bias but high variance and poor generalization to new data.\n",
        "*   **Suboptimal performance:** Even without severe underfitting or overfitting, a model with default or poorly chosen hyperparameters will likely not achieve its full potential. Tuning allows us to maximize the model's predictive power for a given dataset and task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f16649"
      },
      "source": [
        "### 2. Common Hyperparameter Tuning Strategies\n",
        "\n",
        "Two widely used strategies for hyperparameter tuning are `GridSearchCV` and `RandomizedSearchCV`.\n",
        "\n",
        "#### a. GridSearchCV\n",
        "\n",
        "**Concept:** `GridSearchCV` exhaustively searches over a specified subset of the hyperparameter space. For every combination of hyperparameters provided in a grid, it trains and evaluates the model using cross-validation. The combination that yields the best performance is selected.\n",
        "\n",
        "**Pros:**\n",
        "*   **Exhaustive Search:** Guarantees finding the optimal combination within the defined search space.\n",
        "*   **Reproducible:** The search is deterministic, meaning it will always yield the same result for the same grid.\n",
        "\n",
        "**Cons:**\n",
        "*   **Computationally Expensive:** Can be very time-consuming, especially with many hyperparameters or a large range of values, as it explores *every* possible combination.\n",
        "*   **Scalability Issues:** Becomes impractical when the search space (number of hyperparameters * number of values per hyperparameter) is large.\n",
        "*   **Inefficient for High-Dimensional Spaces:** Often wastes time evaluating unpromising regions of the search space.\n",
        "\n",
        "**Applicability to CatBoost:** While effective for a small number of hyperparameters, `GridSearchCV` can quickly become prohibitive for CatBoost due to its relatively large number of influential hyperparameters (e.g., `iterations`, `learning_rate`, `depth`, `l2_leaf_reg`, `bagging_temperature`, `random_strength`). An exhaustive search over all these parameters would likely take too long.\n",
        "\n",
        "#### b. RandomizedSearchCV\n",
        "\n",
        "**Concept:** `RandomizedSearchCV` samples a fixed number of hyperparameter settings from specified distributions. Instead of trying every combination, it tries `n_iter` random combinations. This allows for a more efficient exploration of a larger search space.\n",
        "\n",
        "**Pros:**\n",
        "*   **Computational Efficiency:** Significantly faster than `GridSearchCV` for large search spaces, as it doesn't explore every combination.\n",
        "*   **Better Exploration:** Can find good solutions more quickly by exploring a wider range of the search space, especially when only a few hyperparameters are truly important.\n",
        "*   **Scalable:** More suitable for models with many hyperparameters and continuous parameter ranges.\n",
        "\n",
        "**Cons:**\n",
        "*   **Not Exhaustive:** Does not guarantee finding the absolute best combination within the search space, as it relies on random sampling.\n",
        "*   **Requires Distributions:** Needs a statistical distribution or a list of possible values for each hyperparameter.\n",
        "\n",
        "**Applicability to CatBoost:** `RandomizedSearchCV` is generally a more suitable choice for CatBoost. It allows for efficient exploration of CatBoost's rich hyperparameter space without the prohibitive computational cost of an exhaustive grid search. This is particularly useful for parameters like `learning_rate` or `l2_leaf_reg` which can be sampled from a continuous distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1796a0b0"
      },
      "source": [
        "### 3. Recommended Tuning Strategy for Loan Default Prediction\n",
        "\n",
        "Given the complexity of CatBoost and the nature of the loan default prediction problem (which typically involves a medium to large dataset with a mix of numerical and categorical features, and often class imbalance), **RandomizedSearchCV is the most suitable hyperparameter tuning strategy**.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "1.  **Efficiency for CatBoost:** CatBoost has numerous influential hyperparameters. An exhaustive `GridSearchCV` would be computationally prohibitive and time-consuming, especially when exploring multiple hyperparameters simultaneously. `RandomizedSearchCV` allows for efficient exploration of a larger search space within a reasonable time budget.\n",
        "2.  **Handling of Imbalanced Data:** Loan default datasets are typically highly imbalanced (far fewer defaults than non-defaults). Tuning hyperparameters is critical to ensure the model doesn't simply predict the majority class. `RandomizedSearchCV` can efficiently explore parameter combinations that help address this imbalance (e.g., `class_weights`, `scale_pos_weight`).\n",
        "3.  **Complex Feature Interactions:** Boosting models like CatBoost can capture complex non-linear relationships and interactions. Effective tuning is needed to prevent overfitting while allowing the model to learn these patterns. `RandomizedSearchCV` is better suited for efficiently searching the high-dimensional hyperparameter space that arises from such models.\n",
        "4.  **Flexibility for Distributions:** Many CatBoost hyperparameters (e.g., `learning_rate`, `l2_leaf_reg`, `bagging_temperature`) are best sampled from continuous distributions, which `RandomizedSearchCV` handles naturally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0e84e5"
      },
      "source": [
        "### 4. Important Hyperparameters for CatBoost Classifier\n",
        "\n",
        "When tuning a CatBoost Classifier, especially for a problem like loan default prediction with imbalanced data and a mix of feature types, the following hyperparameters are particularly important:\n",
        "\n",
        "1.  **`iterations`** (or `num_boost_round`, `n_estimators`):\n",
        "    *   **Description:** The maximum number of trees to build. More iterations can lead to a more complex model and potentially better fit, but also a higher risk of overfitting.\n",
        "    *   **Tuning Consideration:** Often tuned in conjunction with `early_stopping_rounds` to find the optimal number of trees without overfitting.\n",
        "\n",
        "2.  **`learning_rate`** (or `eta`):\n",
        "    *   **Description:** Determines the step size at each iteration when updating the model weights. A smaller learning rate requires more iterations but can lead to a more robust model. A larger learning rate might converge faster but risk overshooting the optimum.\n",
        "    *   **Tuning Consideration:** Crucial for convergence and model performance. Often tuned on a logarithmic scale (e.g., `[0.01, 0.05, 0.1, 0.2]`).\n",
        "\n",
        "3.  **`depth`** (or `max_depth`):\n",
        "    *   **Description:** The maximum depth of the individual decision trees. Deeper trees can model more complex relationships but increase the risk of overfitting. Shallower trees are more robust to noise.\n",
        "    *   **Tuning Consideration:** Controls model complexity. Typically in the range of 4 to 10 for many problems.\n",
        "\n",
        "4.  **`l2_leaf_reg`** (or `lambda`):\n",
        "    *   **Description:** The L2 regularization coefficient applied to the leaf values of the trees. It helps prevent overfitting by penalizing large weights.\n",
        "    *   **Tuning Consideration:** A higher value makes the model more conservative. Can be tuned on a logarithmic scale.\n",
        "\n",
        "5.  **`random_strength`**:\n",
        "    *   **Description:** The amount of randomness to use for scoring splits. A higher value can help prevent overfitting, especially when dealing with many categorical features or noisy data.\n",
        "    *   **Tuning Consideration:** Can be useful for regularization, especially in complex datasets.\n",
        "\n",
        "6.  **`bagging_temperature`**:\n",
        "    *   **Description:** Defines the intensity of Bayesian bagging. The higher the temperature, the more aggressive the bagging. This parameter helps to reduce overfitting by introducing randomness into the training process.\n",
        "    *   **Tuning Consideration:** A value of `1` often works well. Can be tuned to control variance.\n",
        "\n",
        "7.  **`class_weights`** or **`auto_class_weights`**:\n",
        "    *   **Description:** Essential for handling imbalanced datasets. `class_weights` allows you to assign specific weights to each class, giving more importance to the minority class. `auto_class_weights` can automatically calculate weights based on class frequencies.\n",
        "    *   **Tuning Consideration:** Crucial for loan default prediction. Can significantly improve the model's ability to detect defaults without sacrificing overall performance too much. For binary classification, `scale_pos_weight` can also be used to scale the weight of the positive class.\n",
        "\n",
        "8.  **`early_stopping_rounds`** (or `patience`):\n",
        "    *   **Description:** A technique to stop training early if the model's performance on a validation set does not improve for a certain number of consecutive iterations. This prevents overfitting and saves computational resources.\n",
        "    *   **Tuning Consideration:** While not a hyperparameter in the sense of model complexity, it's a critical training parameter that should be used during tuning to determine the optimal `iterations` implicitly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc5858c"
      },
      "source": [
        "### 5. Cross-Validation Considerations\n",
        "\n",
        "Cross-validation is a critical component of any robust hyperparameter tuning strategy. It ensures that the model's performance estimates are reliable and that the chosen hyperparameters generalize well to unseen data.\n",
        "\n",
        "**How Cross-Validation Works:**\n",
        "\n",
        "1.  **Data Partitioning:** The training dataset is split into `k` smaller subsets (folds). A common choice is `k=5` or `k=10`.\n",
        "2.  **Iterative Training and Validation:** The model is trained `k` times. In each iteration, one fold is used as the validation set, and the remaining `k-1` folds are used as the training set.\n",
        "3.  **Performance Aggregation:** The performance metric (e.g., accuracy, F1-score, AUC-ROC) is calculated for each of the `k` validation runs. The average and standard deviation of these `k` scores provide a more robust estimate of the model's performance for a given set of hyperparameters.\n",
        "\n",
        "**Importance within Hyperparameter Tuning (e.g., with `RandomizedSearchCV`):**\n",
        "\n",
        "*   **Robust Performance Estimation:** When `RandomizedSearchCV` (or `GridSearchCV`) is used, it internally performs cross-validation for each set of sampled hyperparameters. This provides a less biased estimate of how well a particular hyperparameter combination will perform on new data, compared to a single train-validation split.\n",
        "*   **Prevents Overfitting to the Validation Set:** If we were to use a single fixed validation set to choose hyperparameters, there's a risk of tuning the model specifically to the peculiarities of that single validation set. This can lead to hyperparameters that perform well on *that specific* validation set but poorly on truly unseen data. Cross-validation averages performance across multiple validation sets, making the hyperparameter selection more generalizable.\n",
        "*   **Efficient Use of Data:** Cross-validation allows us to use all available data for both training and validation over the course of the tuning process. Each data point gets a chance to be in a validation set exactly once and in a training set `k-1` times.\n",
        "*   **Identifies Stable Hyperparameters:** By observing the variance of performance across folds, one can identify hyperparameters that lead to more stable model performance, rather than those that might just perform exceptionally well on one specific split by chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1714b83a"
      },
      "source": [
        "### 5. Cross-Validation Considerations\n",
        "\n",
        "Cross-validation is a critical component of any robust hyperparameter tuning strategy. It ensures that the model's performance estimates are reliable and that the chosen hyperparameters generalize well to unseen data.\n",
        "\n",
        "**How Cross-Validation Works:**\n",
        "\n",
        "1.  **Data Partitioning:** The training dataset is split into `k` smaller subsets (folds). A common choice is `k=5` or `k=10`.\n",
        "2.  **Iterative Training and Validation:** The model is trained `k` times. In each iteration, one fold is used as the validation set, and the remaining `k-1` folds are used as the training set.\n",
        "3.  **Performance Aggregation:** The performance metric (e.g., accuracy, F1-score, AUC-ROC) is calculated for each of the `k` validation runs. The average and standard deviation of these `k` scores provide a more robust estimate of the model's performance for a given set of hyperparameters.\n",
        "\n",
        "**Importance within Hyperparameter Tuning (e.g., with `RandomizedSearchCV`):**\n",
        "\n",
        "*   **Robust Performance Estimation:** When `RandomizedSearchCV` (or `GridSearchCV`) is used, it internally performs cross-validation for each set of sampled hyperparameters. This provides a less biased estimate of how well a particular hyperparameter combination will perform on new data, compared to a single train-validation split.\n",
        "*   **Prevents Overfitting to the Validation Set:** If we were to use a single fixed validation set to choose hyperparameters, there's a risk of tuning the model specifically to the peculiarities of that single validation set. This can lead to hyperparameters that perform well on *that specific* validation set but poorly on truly unseen data. Cross-validation averages performance across multiple validation sets, making the hyperparameter selection more generalizable.\n",
        "*   **Efficient Use of Data:** Cross-validation allows us to use all available data for both training and validation over the course of the tuning process. Each data point gets a chance to be in a validation set exactly once and in a training set `k-1` times.\n",
        "*   **Identifies Stable Hyperparameters:** By observing the variance of performance across folds, one can identify hyperparameters that lead to more stable model performance, rather than those that might just perform exceptionally well on one specific split by chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359d41b7"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "### Subtask:\n",
        "Detail appropriate evaluation metrics for an imbalanced classification problem like loan default prediction, explaining their suitability and business implications.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "1.  **Why Accuracy is Insufficient for Imbalanced Classification:**\n",
        "    In imbalanced classification problems, such as loan default prediction where non-default cases (majority class) significantly outnumber default cases (minority class), simple accuracy can be highly misleading. If 95% of loans are repaid and only 5% default, a model that simply predicts 'no default' for every applicant would achieve 95% accuracy. While seemingly high, this model is useless as it fails to identify any defaulting customers, which is the primary business concern. The majority class heavily skews the accuracy metric, making it an unreliable indicator of a model's true performance, especially its ability to detect the rare, but critical, events.\n",
        "\n",
        "2.  **Precision, Recall, and F1-score:**\n",
        "    These metrics are crucial for evaluating performance on the minority class and understanding the trade-offs in classification.\n",
        "    *   **Precision:** Measures the proportion of positive identifications that were actually correct. In loan default prediction, it answers: \"Of all the customers predicted to default, how many actually defaulted?\" High precision means fewer false positives (classifying a good borrower as a defaulter).\n",
        "    *   **Recall (Sensitivity):** Measures the proportion of actual positives that were identified correctly. In loan default prediction, it answers: \"Of all the customers who actually defaulted, how many did the model correctly identify?\" High recall means fewer false negatives (missing actual defaulters).\n",
        "    *   **F1-score:** The harmonic mean of Precision and Recall. It provides a single score that balances both precision and recall, being particularly useful when there is an uneven class distribution and you want to balance the cost of false positives and false negatives.\n",
        "\n",
        "    **Trade-off between Precision and Recall:**\n",
        "    There's often a trade-off between precision and recall. A model tuned for very high recall might incorrectly flag many non-defaulters as defaulters (low precision). Conversely, a model tuned for very high precision might miss many actual defaulters (low recall). In financial risk, this trade-off is critical. For instance, a bank might prefer higher recall to minimize the risk of approving loans to defaulters, even if it means rejecting some good customers (higher false positives). The optimal balance depends on the specific business context and the relative costs of false positives versus false negatives.\n",
        "\n",
        "3.  **ROC-AUC and AUPRC (Area Under the Precision-Recall Curve):**\n",
        "    These provide a more comprehensive view of a classifier's performance across various threshold settings.\n",
        "    *   **ROC-AUC (Receiver Operating Characteristic - Area Under the Curve):** The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate at various classification thresholds. The AUC (Area Under the Curve) measures the overall ability of the model to distinguish between classes. A higher AUC indicates better discrimination. ROC-AUC is generally robust to class imbalance, as it considers both false positives and true positives.\n",
        "    *   **AUPRC (Area Under the Precision-Recall Curve):** The Precision-Recall curve plots Precision against Recall at various classification thresholds. The AUPRC measures the average precision (weighted by the increase in recall) across all possible thresholds. For highly imbalanced datasets, AUPRC is often more informative than ROC-AUC. This is because ROC curves can be misleadingly optimistic on imbalanced datasets when the number of true negatives is very high, making the False Positive Rate (FP / (FP + TN)) remain low even if the model performs poorly on the minority class. AUPRC, by focusing solely on the positive class and its predictions, provides a clearer picture of the model's ability to identify the minority class without being diluted by the large number of correctly predicted majority class instances.\n",
        "\n",
        "4.  **Business Implications of False Positives and False Negatives:**\n",
        "    In loan default prediction, the costs associated with misclassifications are substantial and have direct business implications:\n",
        "    *   **False Positive (Type I Error):** A customer who would have repaid their loan is incorrectly classified as a defaulter and thus denied a loan.\n",
        "        *   **Business Implication:** Loss of potential revenue (interest payments), damaged customer relationships, and competitive disadvantage. The bank misses out on a profitable opportunity. The cost is opportunity cost.\n",
        "    *   **False Negative (Type II Error):_** A customer who will actually default is incorrectly classified as a non-defaulter and granted a loan.\n",
        "        *   **Business Implication:** Direct financial loss (unrecovered loan amount), increased risk exposure for the bank, and potential regulatory penalties. The cost is a direct financial loss.\n",
        "\n",
        "    The choice of metrics and the model's tuning (e.g., threshold adjustment) should directly align with the bank's risk appetite and strategic goals. If avoiding financial losses from defaults is paramount, the bank might prioritize recall (minimizing false negatives) even at the expense of some false positives. Conversely, if acquiring new customers and maximizing loan disbursements is key, a balance might be struck where precision is also highly valued. Boosting techniques, by inherently focusing on correcting errors and often performing well on imbalanced data, are well-suited to optimizing these metrics to meet specific business objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b79f5dc0"
      },
      "source": [
        "## Business Benefits\n",
        "\n",
        "### Subtask:\n",
        "Articulate how a robust loan default prediction model can benefit the FinTech company.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4c8c307"
      },
      "source": [
        "A robust loan default prediction model can bring significant benefits to a FinTech company:\n",
        "\n",
        "1.  **Reducing Financial Losses:** By accurately identifying potential defaulters, the model allows the FinTech company to decline high-risk loan applications or offer loans with adjusted terms (e.g., higher interest rates, shorter repayment periods, or requiring collateral) to mitigate risk. This proactive approach directly translates into a significant reduction in charge-offs and overall financial losses due to loan defaults.\n",
        "\n",
        "2.  **Optimizing Loan Approval Processes:** The model automates and streamlines the credit assessment process, making it faster and more consistent. This efficiency means quicker loan decisions for applicants, which improves customer experience, and allows the company to process a higher volume of applications with fewer manual errors. The data-driven decisions lead to more reliable and objective outcomes, reducing bias.\n",
        "\n",
        "3.  **Improving Overall Risk Assessment:** The model provides deeper insights into the risk profile of the loan portfolio. It can identify patterns and risk factors that might be overlooked by traditional methods. This enhanced understanding of risk enables better capital allocation, more accurate provisioning for potential losses, and more informed strategic business decisions regarding market expansion or product development.\n",
        "\n",
        "4.  **Enhanced Customer Segmentation:** With accurate default predictions, the company can segment customers more effectively. High-risk customers can be managed with specific strategies, while low-risk, high-potential customers can be targeted with tailored offerings, promotional rates, or higher credit limits. This leads to more personalized customer experiences, improved customer loyalty, and ultimately, better customer relationship management and lifetime value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb7e8253"
      },
      "source": [
        "## Python Code Example\n",
        "\n",
        "### Subtask:\n",
        "Provide a comprehensive Python code example demonstrating the data science pipeline for loan default prediction, including synthetic data generation with imbalance, mixed features, and missing values, basic preprocessing, CatBoost model training, and evaluation with appropriate metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dce84e4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires generating a comprehensive Python code example for a loan default prediction pipeline. The first step is to import all necessary libraries to perform data generation, preprocessing, model training, and evaluation as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd73ccc7",
        "outputId": "d6017469-f922-4f77-c00b-6526a8a9cb67"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6eede11"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to generate a synthetic dataset as per the instructions, including numerical and categorical features, missing values, and an imbalanced target variable. This will simulate a real-world loan default prediction scenario.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "829fc2e8",
        "outputId": "1addeceb-b912-41e3-dcf7-f3f0cdb50a0f"
      },
      "source": [
        "np.random.seed(42)\n",
        "num_samples = 1500\n",
        "\n",
        "# Generate numerical features\n",
        "data = {\n",
        "    'Age': np.random.randint(20, 65, num_samples),\n",
        "    'Income': np.random.normal(50000, 15000, num_samples).astype(int).clip(15000, 150000),\n",
        "    'LoanAmount': np.random.normal(15000, 8000, num_samples).astype(int).clip(1000, 50000),\n",
        "    'CreditScore': np.random.randint(300, 850, num_samples)\n",
        "}\n",
        "\n",
        "# Generate categorical features\n",
        "education_levels = ['High School', 'Bachelors', 'Masters', 'PhD']\n",
        "marital_status = ['Single', 'Married', 'Divorced']\n",
        "loan_types = ['Personal Loan', 'Auto Loan', 'Home Loan', 'Student Loan']\n",
        "employment_status = ['Employed', 'Unemployed', 'Self-EmEmployed', 'Retired']\n",
        "\n",
        "data['Education'] = np.random.choice(education_levels, num_samples)\n",
        "data['MaritalStatus'] = np.random.choice(marital_status, num_samples)\n",
        "data['LoanType'] = np.random.choice(loan_types, num_samples)\n",
        "data['EmploymentStatus'] = np.random.choice(employment_status, num_samples)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create imbalanced target variable (Default: 10% defaults)\n",
        "default_ratio = 0.10\n",
        "num_defaults = int(num_samples * default_ratio)\n",
        "num_non_defaults = num_samples - num_defaults\n",
        "\n",
        "df['Default'] = [1] * num_defaults + [0] * num_non_defaults\n",
        "np.random.shuffle(df['Default'].values) # Shuffle to mix default and non-default cases\n",
        "\n",
        "# Introduce random missing values (approx 10-15%)\n",
        "missing_rate = 0.12\n",
        "for col in ['Age', 'Income', 'LoanAmount', 'CreditScore', 'Education', 'MaritalStatus']:\n",
        "    missing_indices = np.random.choice(df.index, int(num_samples * missing_rate), replace=False)\n",
        "    df.loc[missing_indices, col] = np.nan\n",
        "\n",
        "print(f\"Generated synthetic dataset with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "print(f\"Default class distribution:\\n{df['Default'].value_counts(normalize=True)}\")\n",
        "print(f\"Missing values overview:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated synthetic dataset with 1500 rows and 9 columns.\n",
            "Default class distribution:\n",
            "Default\n",
            "0    0.9\n",
            "1    0.1\n",
            "Name: proportion, dtype: float64\n",
            "Missing values overview:\n",
            "Age              180\n",
            "Income           180\n",
            "LoanAmount       180\n",
            "CreditScore      180\n",
            "Education        180\n",
            "MaritalStatus    180\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Age   Income  LoanAmount  CreditScore    Education MaritalStatus  \\\n",
              "0  58.0  67506.0     16949.0        793.0    Bachelors       Married   \n",
              "1  48.0  74316.0     26094.0        486.0          NaN           NaN   \n",
              "2  34.0  26450.0      7028.0        797.0      Masters      Divorced   \n",
              "3  62.0  51109.0     12761.0        741.0  High School       Married   \n",
              "4  27.0  42236.0         NaN        429.0  High School       Married   \n",
              "\n",
              "       LoanType EmploymentStatus  Default  \n",
              "0     Auto Loan          Retired        0  \n",
              "1     Auto Loan          Retired        0  \n",
              "2     Home Loan         Employed        0  \n",
              "3  Student Loan          Retired        0  \n",
              "4     Home Loan          Retired        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-522ce561-9871-4261-8f42-305fa8c83266\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Income</th>\n",
              "      <th>LoanAmount</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Education</th>\n",
              "      <th>MaritalStatus</th>\n",
              "      <th>LoanType</th>\n",
              "      <th>EmploymentStatus</th>\n",
              "      <th>Default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58.0</td>\n",
              "      <td>67506.0</td>\n",
              "      <td>16949.0</td>\n",
              "      <td>793.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>Married</td>\n",
              "      <td>Auto Loan</td>\n",
              "      <td>Retired</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48.0</td>\n",
              "      <td>74316.0</td>\n",
              "      <td>26094.0</td>\n",
              "      <td>486.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Auto Loan</td>\n",
              "      <td>Retired</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.0</td>\n",
              "      <td>26450.0</td>\n",
              "      <td>7028.0</td>\n",
              "      <td>797.0</td>\n",
              "      <td>Masters</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Home Loan</td>\n",
              "      <td>Employed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62.0</td>\n",
              "      <td>51109.0</td>\n",
              "      <td>12761.0</td>\n",
              "      <td>741.0</td>\n",
              "      <td>High School</td>\n",
              "      <td>Married</td>\n",
              "      <td>Student Loan</td>\n",
              "      <td>Retired</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27.0</td>\n",
              "      <td>42236.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>429.0</td>\n",
              "      <td>High School</td>\n",
              "      <td>Married</td>\n",
              "      <td>Home Loan</td>\n",
              "      <td>Retired</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-522ce561-9871-4261-8f42-305fa8c83266')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-522ce561-9871-4261-8f42-305fa8c83266 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-522ce561-9871-4261-8f42-305fa8c83266');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4c0c2b9e-54c1-4d37-83e3-766877e79d9e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c0c2b9e-54c1-4d37-83e3-766877e79d9e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4c0c2b9e-54c1-4d37-83e3-766877e79d9e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1500,\n  \"fields\": [\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.927092651633552,\n        \"min\": 20.0,\n        \"max\": 64.0,\n        \"num_unique_values\": 45,\n        \"samples\": [\n          60.0,\n          28.0,\n          37.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14323.56322086312,\n        \"min\": 15000.0,\n        \"max\": 94475.0,\n        \"num_unique_values\": 1297,\n        \"samples\": [\n          36709.0,\n          54715.0,\n          16399.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LoanAmount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7634.522345587947,\n        \"min\": 1000.0,\n        \"max\": 39495.0,\n        \"num_unique_values\": 1247,\n        \"samples\": [\n          19187.0,\n          28966.0,\n          10525.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CreditScore\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 160.1470465407522,\n        \"min\": 300.0,\n        \"max\": 849.0,\n        \"num_unique_values\": 498,\n        \"samples\": [\n          817.0,\n          758.0,\n          724.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Masters\",\n          \"PhD\",\n          \"Bachelors\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MaritalStatus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Married\",\n          \"Divorced\",\n          \"Single\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LoanType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Home Loan\",\n          \"Personal Loan\",\n          \"Auto Loan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EmploymentStatus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Employed\",\n          \"Unemployed\",\n          \"Retired\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ff451c4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the synthetic dataset is generated, the next step is to identify the categorical features and store their indices. CatBoost requires this information to properly handle categorical columns natively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "115601ce",
        "outputId": "6725b449-cb98-4971-d32b-9fc5876e4c23"
      },
      "source": [
        "categorical_features = ['Education', 'MaritalStatus', 'LoanType', 'EmploymentStatus']\n",
        "cat_features_indices = [df.columns.get_loc(col) for col in categorical_features]\n",
        "\n",
        "print(f\"Identified categorical features: {categorical_features}\")\n",
        "print(f\"Categorical feature indices: {cat_features_indices}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified categorical features: ['Education', 'MaritalStatus', 'LoanType', 'EmploymentStatus']\n",
            "Categorical feature indices: [4, 5, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35cf9629"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to split the DataFrame into features (X) and target (y), and then divide these into training and testing sets, as per instruction #4.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0579d56e",
        "outputId": "3b88c350-2b29-4e5d-f79d-f4aa5929fd00"
      },
      "source": [
        "X = df.drop('Default', axis=1)\n",
        "y = df['Default']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1200, 8)\n",
            "X_test shape: (300, 8)\n",
            "y_train shape: (1200,)\n",
            "y_test shape: (300,)\n",
            "Train target distribution:\n",
            "Default\n",
            "0    0.9\n",
            "1    0.1\n",
            "Name: proportion, dtype: float64\n",
            "Test target distribution:\n",
            "Default\n",
            "0    0.9\n",
            "1    0.1\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aec8809e"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to handle missing numerical values using `SimpleImputer` with a median strategy, as specified in instruction #5, and then transform both training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bbe158",
        "outputId": "f998f9c3-2727-4e11-81cb-8c45e04b2bd9"
      },
      "source": [
        "numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
        "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
        "\n",
        "print(\"Missing numerical values imputed using median strategy.\")\n",
        "print(f\"Missing values in X_train after imputation:\\n{X_train.isnull().sum()[X_train.isnull().sum() > 0]}\")\n",
        "print(f\"Missing values in X_test after imputation:\\n{X_test.isnull().sum()[X_test.isnull().sum() > 0]}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing numerical values imputed using median strategy.\n",
            "Missing values in X_train after imputation:\n",
            "Education        148\n",
            "MaritalStatus    145\n",
            "dtype: int64\n",
            "Missing values in X_test after imputation:\n",
            "Education        32\n",
            "MaritalStatus    35\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fc9061"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to calculate class weights for the imbalanced target variable `y_train` using `sklearn.utils.class_weight.compute_class_weight` to help the CatBoost model handle the class imbalance effectively, as specified in instruction #6.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a809021",
        "outputId": "06ad6064-6297-42dd-898f-bc75c2bd751c"
      },
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(f\"Calculated class weights: {class_weights_dict}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated class weights: {0: np.float64(0.5555555555555556), 1: np.float64(5.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c6e626"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to initialize and train the CatBoostClassifier with the calculated class weights and categorical feature indices, as specified in instruction #7. This will set up the model for loan default prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c9d8db",
        "outputId": "00fe9517-9338-4772-f5d9-17f609eb67dc"
      },
      "source": [
        "cat_clf = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=0, # Suppress training output\n",
        "    class_weights=class_weights_dict,\n",
        "    early_stopping_rounds=50 # Add early stopping to prevent overfitting\n",
        ")\n",
        "\n",
        "print(\"Starting CatBoost Classifier training...\")\n",
        "# Fit the model to the training data, explicitly passing categorical feature indices\n",
        "cat_clf.fit(X_train, y_train, cat_features=cat_features_indices, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
        "\n",
        "print(\"CatBoost Classifier trained successfully.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting CatBoost Classifier training...\n",
            "CatBoost Classifier trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53b68564"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because CatBoost encountered NaN values in categorical features, which it expects to be strings or integers. Although CatBoost can handle missing values natively if they are explicitly designated as such, the current state has actual `np.nan` values in categorical columns. Therefore, before training the CatBoost model, I need to impute the missing categorical values with a suitable placeholder, such as the mode of each categorical column, to ensure all categorical features are valid strings or integers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01fa8455",
        "outputId": "3ca2c421-4845-4c62-a0a6-5873046885c9"
      },
      "source": [
        "numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "categorical_cols_with_missing = [col for col in categorical_features if X[col].isnull().any()] # Identify categorical columns that might have NaNs\n",
        "\n",
        "imputer_numerical = SimpleImputer(strategy='median')\n",
        "\n",
        "X_train[numerical_cols] = imputer_numerical.fit_transform(X_train[numerical_cols])\n",
        "X_test[numerical_cols] = imputer_numerical.transform(X_test[numerical_cols])\n",
        "\n",
        "# Impute missing categorical values with the mode\n",
        "for col in categorical_cols_with_missing:\n",
        "    mode_value_train = X_train[col].mode()[0] # Get mode from training data\n",
        "    X_train[col].fillna(mode_value_train, inplace=True)\n",
        "    X_test[col].fillna(mode_value_train, inplace=True) # Use training mode for test set\n",
        "\n",
        "print(\"Missing numerical values imputed using median strategy.\")\n",
        "print(\"Missing categorical values imputed using mode strategy.\")\n",
        "print(\"Missing values in X_train after imputation:{X_train.isnull().sum()[X_train.isnull().sum() > 0]}\")\n",
        "print(\"Missing values in X_test after imputation:{X_test.isnull().sum()[X_test.isnull().sum() > 0]}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing numerical values imputed using median strategy.\n",
            "Missing categorical values imputed using mode strategy.\n",
            "Missing values in X_train after imputation:{X_train.isnull().sum()[X_train.isnull().sum() > 0]}\n",
            "Missing values in X_test after imputation:{X_test.isnull().sum()[X_test.isnull().sum() > 0]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-940043818.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(mode_value_train, inplace=True)\n",
            "/tmp/ipython-input-940043818.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(mode_value_train, inplace=True) # Use training mode for test set\n"
          ]
        }
      ]
    }
  ]
}