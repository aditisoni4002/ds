{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "_E8rYFbky36U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "\n",
        "   A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. In the context of classification, it works by recursively splitting the dataset into smaller subsets based on feature values, creating a tree-like structure. Each internal node of the tree represents a \"test\" on an attribute (e.g., is age > 30?), each branch represents the outcome of the test, and each leaf node represents a class label (the decision made after computing all attributes). The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. The splits are chosen to maximize homogeneity within the resulting subsets, often measured by metrics like Gini impurity or information gain.\n",
        "\n"
      ],
      "metadata": {
        "id": "xiyLdKMGy5qh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163efe85"
      },
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity and Entropy are both measures used in Decision Trees to evaluate the quality of a split. They quantify the \"impurity\" or \"disorder\" of a set of samples. The goal during tree construction is to find splits that minimize impurity in the child nodes, leading to more homogeneous subsets.\n",
        "\n",
        "*   **Gini Impurity:**\n",
        "    *   It measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. A Gini impurity of 0 means all samples in the subset belong to the same class (perfect purity). A Gini impurity of 0.5 for a binary classification problem means there's an equal distribution of classes (maximum impurity).\n",
        "    *   **Calculation:** For a node, Gini impurity is calculated as 1 minus the sum of the squared probabilities of each class present in that node. $Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$, where $p_i$ is the proportion of samples belonging to class $i$ and $C$ is the number of classes.\n",
        "\n",
        "*   **Entropy:**\n",
        "    *   Originating from information theory, Entropy measures the average amount of information needed to identify the class of a randomly chosen instance in the set. Like Gini, an Entropy of 0 indicates perfect purity (all samples in one class), while higher entropy values indicate greater disorder or mixed classes.\n",
        "    *   **Calculation:** For a node, Entropy is calculated as the negative sum of the product of the probability of each class and the logarithm (base 2) of that probability. $Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$, where $p_i$ is the proportion of samples belonging to class $i$ and $C$ is the number of classes.\n",
        "\n",
        "**How they impact splits:**\n",
        "\n",
        "Decision Trees use these measures to determine the best feature and split point at each node. The algorithm calculates the impurity for a node before and after a potential split. The difference in impurity (often called **Information Gain** when using Entropy, or simply **Gini Gain** when using Gini Impurity) is used to decide which split is optimal. The split that results in the greatest reduction in impurity (i.e., the highest information/Gini gain) is chosen. This process is recursively applied until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf, or no further impurity reduction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8df0391"
      },
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "Pre-pruning and Post-pruning are two techniques used to prevent overfitting in Decision Trees, which occurs when a tree is too complex and learns the training data too well, leading to poor generalization on unseen data.\n",
        "\n",
        "*   **Pre-Pruning (or Early Stopping):**\n",
        "    *   **What it is:** This technique stops the growth of the decision tree before it has perfectly classified the training data. It sets restrictions on the tree's growth during its construction.\n",
        "    *   **How it works:** Common criteria for pre-pruning include:\n",
        "        *   **Maximum depth:** Limiting how deep the tree can grow.\n",
        "        *   **Minimum samples per leaf:** Requiring a minimum number of samples for a node to be split.\n",
        "        *   **Minimum samples per split:** Requiring a minimum number of samples in a node to consider splitting it.\n",
        "        *   **Minimum impurity decrease:** Stopping a split if the impurity reduction is below a certain threshold.\n",
        "    *   **Practical Advantage:** Pre-pruning is generally **computationally more efficient** than post-pruning, as it avoids building overly complex branches in the first place, saving time and resources during tree construction.\n",
        "\n",
        "*   **Post-Pruning (or Backward Pruning):**\n",
        "    *   **What it is:** This technique allows the decision tree to grow to its full potential (or until it perfectly classifies the training data) and then prunes back the unnecessary branches. It involves removing branches that contribute little to the generalization performance.\n",
        "    *   **How it works:** The process usually involves:\n",
        "        *   Growing a full decision tree.\n",
        "        *   Using a validation set (or cross-validation) to evaluate the performance of the tree after removing certain branches or replacing subtrees with leaf nodes. The goal is to find the subtree that minimizes error on the validation set.\n",
        "        *   Techniques like Reduced Error Pruning or Cost-Complexity Pruning (used in algorithms like CART) are common.\n",
        "    *   **Practical Advantage:** Post-pruning often leads to **more optimal or accurate trees** compared to pre-pruning. Since the tree is allowed to grow fully first, it can capture more complex relationships that might be missed by early stopping criteria. It can then selectively remove only the branches that genuinely contribute to overfitting, rather than prematurely cutting off potentially useful branches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86e21a94"
      },
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Information Gain** is a key metric used in the construction of Decision Trees, particularly those based on the ID3 or C4.5 algorithms (which use Entropy as an impurity measure). It quantifies the expected reduction in entropy (or impurity) caused by splitting a dataset on a given attribute. In simpler terms, it tells us how much 'information' a feature provides about the class label.\n",
        "\n",
        "**How it's calculated:**\n",
        "\n",
        "Information Gain is calculated as the difference between the entropy of the parent node (before the split) and the weighted average of the entropies of the child nodes (after the split). The formula is:\n",
        "\n",
        "$Information Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$\n",
        "\n",
        "Where:\n",
        "*   $S$ is the dataset (parent node).\n",
        "*   $A$ is the attribute (feature) being considered for the split.\n",
        "*   $Values(A)$ is the set of all possible values for attribute $A$.\n",
        "*   $S_v$ is the subset of $S$ for which attribute $A$ has value $v$ (child node).\n",
        "*   $|S_v|$ is the number of samples in $S_v$.\n",
        "*   $|S|$ is the total number of samples in $S$.\n",
        "*   $Entropy(X)$ is the entropy of dataset $X$.\n",
        "\n",
        "**Why it's important for choosing the best split:**\n",
        "\n",
        "Information Gain is crucial because it provides a quantitative way to select the most informative feature to split the data at each node of the Decision Tree. The goal of a Decision Tree is to create pure leaf nodes, meaning nodes where all samples belong to the same class. A higher Information Gain indicates a better split, as it means the split significantly reduces the uncertainty (entropy) about the class labels.\n",
        "\n",
        "The Decision Tree algorithm works by iteratively selecting the attribute that yields the highest Information Gain (or greatest reduction in impurity, if using Gini Impurity for what's sometimes called Gini Gain) at each step. This greedy approach ensures that the tree prioritizes splits that lead to the most homogeneous child nodes, thereby building a tree that can effectively classify or predict the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c0866a0"
      },
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Decision Trees are versatile and widely used machine learning algorithms due to their interpretability and ability to handle various data types. Here are some common real-world applications, along with their main advantages and limitations:\n",
        "\n",
        "**Common Real-World Applications:**\n",
        "\n",
        "1.  **Medical Diagnosis:** Decision trees can assist in diagnosing diseases based on patient symptoms, medical history, and test results. For example, classifying a tumor as benign or malignant.\n",
        "2.  **Credit Scoring and Fraud Detection:** Banks and financial institutions use decision trees to assess the creditworthiness of loan applicants and to identify fraudulent transactions by analyzing patterns in financial data.\n",
        "3.  **Customer Relationship Management (CRM):** Predicting customer churn (who is likely to leave), identifying potential customers for targeted marketing campaigns, and segmenting customers based on their behavior.\n",
        "4.  **Manufacturing Quality Control:** Identifying defects in products or predicting equipment failure based on sensor data and operational parameters.\n",
        "5.  **Bioinformatics:** Analyzing gene expression data to classify different types of cells or diseases.\n",
        "6.  **Recommendation Systems:** While more complex algorithms often take the lead, decision trees can be used for simpler recommendation logic, e.g., recommending products based on a user's past purchases and browsing history.\n",
        "7.  **Image Classification (basic):** In some simpler cases, decision trees can classify images based on extracted features.\n",
        "\n",
        "**Main Advantages:**\n",
        "\n",
        "1.  **Easy to Understand and Interpret:** Decision trees mimic human decision-making, making their logic very intuitive and easy for non-technical stakeholders to understand. The visual representation of the tree is straightforward.\n",
        "2.  **Requires Little Data Preparation:** Unlike some other algorithms (e.g., neural networks), decision trees often require less data preprocessing. They can handle both numerical and categorical data without extensive normalization or scaling.\n",
        "3.  **Can Handle Both Classification and Regression Tasks:** They are adaptable to predict discrete categories (classification) and continuous values (regression).\n",
        "4.  **No Parametric Assumptions:** Decision trees are non-parametric, meaning they don't make assumptions about the underlying distribution of the data (e.g., linearity, normality).\n",
        "5.  **Handles Non-linear Relationships:** They can naturally capture non-linear relationships between features and the target variable.\n",
        "6.  **Feature Selection is Implicit:** Important features are typically at the top of the tree, effectively performing an implicit form of feature selection.\n",
        "\n",
        "**Main Limitations:**\n",
        "\n",
        "1.  **Prone to Overfitting:** Without proper pruning or setting stopping criteria, decision trees can easily become overly complex, capturing noise in the training data and performing poorly on unseen data. This is their most significant disadvantage.\n",
        "2.  **Instability:** Small variations in the data can result in a completely different tree structure. This instability can make them less reliable in some contexts.\n",
        "3.  **Bias towards Dominant Classes:** If the dataset has a highly imbalanced class distribution, decision trees can become biased towards the majority class.\n",
        "4.  **Local Optimality (Greedy Approach):** The greedy nature of the algorithm (making the best split at each step) does not guarantee a globally optimal tree. It may miss better overall solutions that require a less optimal split at an earlier stage.\n",
        "5.  **Difficulty with Complex Relationships:** While they handle non-linear relationships, complex interactions between features might require very deep and intricate trees, leading back to the overfitting problem.\n",
        "6.  **Computationally Expensive (for large, deep trees):** Building very deep trees can become computationally intensive, especially with a large number of features and data points.\n",
        "7.  **\"Axis-Parallel\" Splits:** Traditional decision trees make splits based on single features, creating rectangular decision boundaries. This can be suboptimal for data with complex, diagonal, or curved boundaries, where other algorithms (like SVMs with non-linear kernels) might perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "pE1DGOS20yVd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59a7eaaf",
        "outputId": "454fb38c-781a-45a4-d1c0-b72d5130314c"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "decision_tree_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = decision_tree_model.predict(X_test)\n",
        "\n",
        "# 3. Print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, decision_tree_model.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "787ff054"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Model Accuracy: 1.0000\n",
        "\n",
        "Feature Importances:\n",
        "*   sepal length (cm): 0.0000\n",
        "*   sepal width (cm): 0.0191\n",
        "*   petal length (cm): 0.8933\n",
        "*   petal width (cm): 0.0876"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "ELTvwr-A1E9k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41ce9d5",
        "outputId": "82208a45-f412-4270-91c2-f03cd43d5155"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already loaded from the previous task\n",
        "# (If not, include the loading and splitting code again)\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "# feature_names = iris.feature_names\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train a Decision Tree Classifier with max_depth=3\n",
        "decision_tree_pruned = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "decision_tree_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the pruned tree\n",
        "y_pred_pruned = decision_tree_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_pruned:.4f}\")\n",
        "\n",
        "# 2. Compare its accuracy to a fully-grown tree\n",
        "# The fully-grown tree from Question 6 achieved an accuracy of 1.0000\n",
        "accuracy_fully_grown = 1.0000 # Using the value from previous execution\n",
        "print(f\"Accuracy of a fully-grown Decision Tree (from Q6): {accuracy_fully_grown:.4f}\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if accuracy_pruned < accuracy_fully_grown:\n",
        "    print(f\"The pruned tree (max_depth=3) has slightly lower accuracy than the fully-grown tree. This is expected as pruning aims to prevent overfitting, which might sacrifice some training accuracy for better generalization (though in this specific Iris case, the fully-grown tree still got 1.0 on the test set).\")\n",
        "elif accuracy_pruned == accuracy_fully_grown:\n",
        "    print(f\"The pruned tree (max_depth=3) achieved the same accuracy as the fully-grown tree. This suggests that a depth of 3 was sufficient to capture the necessary patterns for perfect classification on this test set, and further growth might lead to overfitting on more complex datasets.\")\n",
        "else:\n",
        "    print(f\"Surprisingly, the pruned tree (max_depth=3) has higher accuracy than the fully-grown tree. This is unusual for test sets unless random_state was not consistent or the fully-grown tree overfitted.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
            "Accuracy of a fully-grown Decision Tree (from Q6): 1.0000\n",
            "\n",
            "Comparison:\n",
            "The pruned tree (max_depth=3) achieved the same accuracy as the fully-grown tree. This suggests that a depth of 3 was sufficient to capture the necessary patterns for perfect classification on this test set, and further growth might lead to overfitting on more complex datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5b5a51"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
        "Accuracy of a fully-grown Decision Tree (from Q6): 1.0000\n",
        "\n",
        "Comparison:\n",
        "The pruned tree (max_depth=3) achieved the same accuracy as the fully-grown tree. This suggests that a depth of 3 was sufficient to capture the necessary patterns for perfect classification on this test set, and further growth might lead to overfitting on more complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "9nyBVQXZ1SMX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a39bca7",
        "outputId": "49743e50-0d78-4ad6-855d-c4c12f3e75d7"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing # Changed from load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing Dataset (as Boston is deprecated)\n",
        "try:\n",
        "    housing = fetch_california_housing()\n",
        "    X = housing.data\n",
        "    y = housing.target\n",
        "    feature_names = housing.feature_names\n",
        "except Exception as e:\n",
        "    print(f\"Error loading California Housing dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Regressor\n",
        "decision_tree_regressor = DecisionTreeRegressor(random_state=42)\n",
        "decision_tree_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_reg = decision_tree_regressor.predict(X_test_reg)\n",
        "\n",
        "# 3. Print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, decision_tree_regressor.feature_importances_):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280\n",
            "\n",
            "Feature Importances:\n",
            "  MedInc: 0.5235\n",
            "  HouseAge: 0.0521\n",
            "  AveRooms: 0.0494\n",
            "  AveBedrms: 0.0250\n",
            "  Population: 0.0322\n",
            "  AveOccup: 0.1390\n",
            "  Latitude: 0.0900\n",
            "  Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ee22f8f"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Mean Squared Error (MSE): 0.5280\n",
        "\n",
        "Feature Importances:\n",
        "*   MedInc: 0.5235\n",
        "*   HouseAge: 0.0521\n",
        "*   AveRooms: 0.0494\n",
        "*   AveBedrms: 0.0250\n",
        "*   Population: 0.0322\n",
        "*   AveOccup: 0.1390\n",
        "*   Latitude: 0.0900\n",
        "*   Longitude: 0.0888"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "x8Wyi2q91u4Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72b5bc7b",
        "outputId": "04b2ec40-518f-43ce-b938-7f6f758d0729"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already loaded from previous tasks\n",
        "# If not, uncomment and run the following lines:\n",
        "# from sklearn.datasets import load_iris\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# We use 'accuracy' as the scoring metric for classification\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_tuned = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"Accuracy of the best tuned model: {accuracy_tuned:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best tuned model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46cbfd6a"
      },
      "source": [
        "Answer:\n",
        "\n",
        "Best Parameters: {'max_depth': None, 'min_samples_split': 10}\n",
        "Accuracy of the best tuned model: 1.0000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n"
      ],
      "metadata": {
        "id": "zkE0pPlA2aL0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bfe4a447",
        "outputId": "8282ca80-0ef4-4e7f-8cab-74b919094c64"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the dataset (using a common Heart Disease dataset URL)\n",
        "try:\n",
        "    # Using a common Heart Disease dataset from UCI via a direct link\n",
        "    # This specific dataset has 14 features and a target variable.\n",
        "    # Adjust the URL if you have a different dataset or local file.\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "    # Define column names as the dataset does not have a header\n",
        "    # These are standard column names for the Cleveland Heart Disease dataset\n",
        "    columns = [\n",
        "        \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
        "        \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
        "    ]\n",
        "    df = pd.read_csv(url, names=columns, na_values=\"?\")\n",
        "    print(\"Dataset loaded successfully.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please ensure the URL is correct or the file exists if loading locally.\")\n",
        "    # Attempt to load a local file if URL fails or for demonstration\n",
        "    # df = pd.read_csv('heart_disease.csv', names=columns, na_values=\"?\") # Uncomment and adjust if you have a local file\n",
        "    exit()\n",
        "\n",
        "# 2. Display the first few rows of the DataFrame\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# 3. Examine the DataFrame's information (data types, non-null counts)\n",
        "print(\"DataFrame Info:\")\n",
        "df.info()\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# 4. Check the dimensions of the dataset (number of rows and columns)\n",
        "print(f\"Dataset shape (rows, columns): {df.shape}\")\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# 5. Generate descriptive statistics for numerical columns\n",
        "print(\"Descriptive statistics for numerical columns:\")\n",
        "print(df.describe())\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# 6. Identify and count missing values for each column\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
            "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
            "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
            "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
            "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
            "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
            "\n",
            "   slope   ca  thal  target  \n",
            "0    3.0  0.0   6.0       0  \n",
            "1    2.0  3.0   3.0       2  \n",
            "2    2.0  2.0   7.0       1  \n",
            "3    3.0  0.0   3.0       0  \n",
            "4    1.0  0.0   3.0       0  \n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    float64\n",
            " 1   sex       303 non-null    float64\n",
            " 2   cp        303 non-null    float64\n",
            " 3   trestbps  303 non-null    float64\n",
            " 4   chol      303 non-null    float64\n",
            " 5   fbs       303 non-null    float64\n",
            " 6   restecg   303 non-null    float64\n",
            " 7   thalach   303 non-null    float64\n",
            " 8   exang     303 non-null    float64\n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    float64\n",
            " 11  ca        299 non-null    float64\n",
            " 12  thal      301 non-null    float64\n",
            " 13  target    303 non-null    int64  \n",
            "dtypes: float64(13), int64(1)\n",
            "memory usage: 33.3 KB\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Dataset shape (rows, columns): (303, 14)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Descriptive statistics for numerical columns:\n",
            "              age         sex          cp    trestbps        chol         fbs  \\\n",
            "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
            "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
            "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
            "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
            "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
            "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
            "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
            "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
            "\n",
            "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
            "count  303.000000  303.000000  303.000000  303.000000  303.000000  299.000000   \n",
            "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.672241   \n",
            "std      0.994971   22.875003    0.469794    1.161075    0.616226    0.937438   \n",
            "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
            "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
            "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
            "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
            "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
            "\n",
            "             thal      target  \n",
            "count  301.000000  303.000000  \n",
            "mean     4.734219    0.937294  \n",
            "std      1.939706    1.228536  \n",
            "min      3.000000    0.000000  \n",
            "25%      3.000000    0.000000  \n",
            "50%      3.000000    0.000000  \n",
            "75%      7.000000    2.000000  \n",
            "max      7.000000    4.000000  \n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Missing values per column:\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          4\n",
            "thal        2\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62315efc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully identified missing values in the 'ca' and 'thal' columns. As per the overall task, the next logical step is to handle these missing values. I will fill them using an appropriate imputation strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2e4426e",
        "outputId": "3c705fb8-35b6-4fb7-84dc-412edf28e96b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Identify columns with missing values\n",
        "missing_cols = df.columns[df.isnull().any()].tolist()\n",
        "print(f\"Columns with missing values before imputation: {missing_cols}\")\n",
        "\n",
        "# For simplicity, we'll fill numerical missing values with the median.\n",
        "# The 'ca' (number of major vessels) and 'thal' (thallium stress test result) are categorical/ordinal\n",
        "# but represented numerically. Median imputation is a reasonable first approach.\n",
        "\n",
        "for col in missing_cols:\n",
        "    if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n",
        "        median_val = df[col].median()\n",
        "        df[col].fillna(median_val, inplace=True)\n",
        "        print(f\"Missing values in column '{col}' filled with median: {median_val}\")\n",
        "    else:\n",
        "        # For other types, mode imputation might be considered, but these are float/int\n",
        "        print(f\"Column '{col}' is not numeric, skipping imputation or requiring specific handling.\")\n",
        "\n",
        "# Verify that missing values have been handled\n",
        "print(\"\\nMissing values per column after imputation:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with missing values before imputation: ['ca', 'thal']\n",
            "Missing values in column 'ca' filled with median: 0.0\n",
            "Missing values in column 'thal' filled with median: 3.0\n",
            "\n",
            "Missing values per column after imputation:\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          0\n",
            "thal        0\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3256471389.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dfb79fe"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code produced a FutureWarning regarding the use of `inplace=True` with chained assignment. To address this warning and ensure best practices, I will modify the imputation line to assign the result back to the DataFrame column directly, which is the recommended approach for `fillna`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7521cb",
        "outputId": "42fcecba-070e-4402-98e0-cb2ffb2c0c4e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Identify columns with missing values\n",
        "missing_cols = df.columns[df.isnull().any()].tolist()\n",
        "print(f\"Columns with missing values before imputation: {missing_cols}\")\n",
        "\n",
        "# For simplicity, we'll fill numerical missing values with the median.\n",
        "# The 'ca' (number of major vessels) and 'thal' (thallium stress test result) are categorical/ordinal\n",
        "# but represented numerically. Median imputation is a reasonable first approach.\n",
        "\n",
        "for col in missing_cols:\n",
        "    if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n",
        "        median_val = df[col].median()\n",
        "        df[col] = df[col].fillna(median_val)\n",
        "        print(f\"Missing values in column '{col}' filled with median: {median_val}\")\n",
        "    else:\n",
        "        # For other types, mode imputation might be considered, but these are float/int\n",
        "        print(f\"Column '{col}' is not numeric, skipping imputation or requiring specific handling.\")\n",
        "\n",
        "# Verify that missing values have been handled\n",
        "print(\"\\nMissing values per column after imputation:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with missing values before imputation: []\n",
            "\n",
            "Missing values per column after imputation:\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "slope       0\n",
            "ca          0\n",
            "thal        0\n",
            "target      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d7021a"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare the data for model training, I need to convert the identified categorical features into a numerical format. Based on the dataset description, 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', and 'thal' are categorical/ordinal features. I will use one-hot encoding for these columns to prevent the model from misinterpreting their numerical values as having an inherent order where none exists (for nominal categories). The target variable 'target' should also be handled separately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232213ef",
        "outputId": "4f6eacf1-12b9-41a7-eb15-2d91cf4cae91"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# The 'target' column indicates the presence of heart disease (0 = no disease, 1-4 = presence of disease).\n",
        "# For simplicity in this classification task, let's convert it into a binary problem:\n",
        "# 0 = no heart disease, 1 = heart disease (any stage).\n",
        "# First, check unique values in 'target' to understand its range.\n",
        "print(f\"Original unique values in 'target' column: {df['target'].unique()}\")\n",
        "\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "print(f\"Unique values in 'target' column after binarization: {df['target'].unique()}\")\n",
        "\n",
        "# Identify categorical columns that are not binary (already 0/1) and not the target variable.\n",
        "# 'sex', 'fbs', 'exang' are already binary (0/1) and can be treated as numerical for Decision Trees.\n",
        "# 'cp', 'restecg', 'slope', 'ca', 'thal' are categorical/ordinal with more than 2 unique values and need one-hot encoding.\n",
        "\n",
        "categorical_cols = ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
        "\n",
        "print(f\"\\nCategorical columns to one-hot encode: {categorical_cols}\")\n",
        "\n",
        "# Apply one-hot encoding to the identified categorical columns\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=False)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_encoded.drop('target', axis=1)\n",
        "y = df_encoded['target']\n",
        "\n",
        "print(f\"\\nDataFrame shape after encoding: {df_encoded.shape}\")\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of the encoded DataFrame (features and target):\")\n",
        "print(df_encoded.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original unique values in 'target' column: [0 2 1 3 4]\n",
            "Unique values in 'target' column after binarization: [0 1]\n",
            "\n",
            "Categorical columns to one-hot encode: ['cp', 'restecg', 'slope', 'ca', 'thal']\n",
            "\n",
            "DataFrame shape after encoding: (303, 26)\n",
            "Features (X) shape: (303, 25)\n",
            "Target (y) shape: (303,)\n",
            "\n",
            "First 5 rows of the encoded DataFrame (features and target):\n",
            "    age  sex  trestbps   chol  fbs  thalach  exang  oldpeak  target  cp_1.0  \\\n",
            "0  63.0  1.0     145.0  233.0  1.0    150.0    0.0      2.3       0    True   \n",
            "1  67.0  1.0     160.0  286.0  0.0    108.0    1.0      1.5       1   False   \n",
            "2  67.0  1.0     120.0  229.0  0.0    129.0    1.0      2.6       1   False   \n",
            "3  37.0  1.0     130.0  250.0  0.0    187.0    0.0      3.5       0   False   \n",
            "4  41.0  0.0     130.0  204.0  0.0    172.0    0.0      1.4       0   False   \n",
            "\n",
            "   ...  slope_1.0  slope_2.0  slope_3.0  ca_0.0  ca_1.0  ca_2.0  ca_3.0  \\\n",
            "0  ...      False      False       True    True   False   False   False   \n",
            "1  ...      False       True      False   False   False   False    True   \n",
            "2  ...      False       True      False   False   False    True   False   \n",
            "3  ...      False      False       True    True   False   False   False   \n",
            "4  ...       True      False      False    True   False   False   False   \n",
            "\n",
            "   thal_3.0  thal_6.0  thal_7.0  \n",
            "0     False      True     False  \n",
            "1      True     False     False  \n",
            "2     False     False      True  \n",
            "3      True     False     False  \n",
            "4      True     False     False  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73b1453d"
      },
      "source": [
        "## Train a Decision Tree Model\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets, then train a Decision Tree Classifier on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47f45899"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been preprocessed (missing values handled, categorical features encoded) and separated into features (X) and target (y). The next logical step is to split the data into training and testing sets and then train a Decision Tree Classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cade21ac",
        "outputId": "38e90289-3eaf-46b5-e941-127074d4974b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# We'll use a 70/30 split, which is a common practice.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training features shape: {X_train.shape}\")\n",
        "print(f\"Testing features shape: {X_test.shape}\")\n",
        "print(f\"Training target shape: {y_train.shape}\")\n",
        "print(f\"Testing target shape: {y_test.shape}\")\n",
        "\n",
        "# Initialize and train a Decision Tree Classifier\n",
        "# Using a random_state for reproducibility\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nDecision Tree Classifier trained successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (212, 25)\n",
            "Testing features shape: (91, 25)\n",
            "Training target shape: (212,)\n",
            "Testing target shape: (91,)\n",
            "\n",
            "Decision Tree Classifier trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461b9f85"
      },
      "source": [
        "## Tune Model Hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Tune the hyperparameters of the Decision Tree Classifier using techniques like GridSearchCV or RandomizedSearchCV to find the optimal set of parameters that improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34f94a77"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been preprocessed, split, and an initial Decision Tree Classifier has been trained. The next step is to tune the hyperparameters of this model. I will use `GridSearchCV` to systematically search for the best combination of `max_depth`, `min_samples_split`, and `min_samples_leaf` to optimize the model's performance on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8423b58f",
        "outputId": "8f16bc91-178d-455b-bbbc-74e48a17bf48"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# We'll use 5-fold cross-validation and 'accuracy' as the scoring metric.\n",
        "# n_jobs=-1 uses all available processors.\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model) from GridSearchCV\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "print(\"\\nHyperparameter tuning complete. Best model obtained.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters found: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
            "\n",
            "Hyperparameter tuning complete. Best model obtained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b69e765f"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the best tuned Decision Tree Classifier on the test set using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b84c7bc8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully tuned the hyperparameters and identified the best parameters. The next logical step is to evaluate the performance of this best-tuned model on the test set using various classification metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "5edf192d",
        "outputId": "1df0f396-1668-42b3-f5f2-78cea1c6f1ba"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make predictions on the test set using the best tuned model\n",
        "y_pred_test = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "precision = precision_score(y_test, y_pred_test, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_test, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "print(f\"\\nModel Evaluation on Test Set (Best Tuned Model):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Print full classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Generate and plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation on Test Set (Best Tuned Model):\n",
            "Accuracy: 0.7363\n",
            "Precision: 0.7399\n",
            "Recall: 0.7363\n",
            "F1-Score: 0.7366\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.71      0.74        49\n",
            "           1       0.70      0.76      0.73        42\n",
            "\n",
            "    accuracy                           0.74        91\n",
            "   macro avg       0.74      0.74      0.74        91\n",
            "weighted avg       0.74      0.74      0.74        91\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7fde9ae"
      },
      "source": [
        "## Business Value of the Model\n",
        "\n",
        "This Decision Tree model, designed to predict the presence of heart disease, offers significant business value to a healthcare company in several ways:\n",
        "\n",
        "1.  **Early Detection and Intervention:** By accurately predicting the likelihood of heart disease, the model can help physicians identify at-risk patients earlier. This allows for timely intervention, lifestyle modifications, or preventative treatments, which can significantly improve patient outcomes and potentially reduce the severity and cost of long-term care.\n",
        "\n",
        "2.  **Resource Optimization:** The model can help in prioritizing healthcare resources. Patients identified as high-risk can be directed to specialized care or diagnostic tests more efficiently, reducing unnecessary tests for low-risk individuals and optimizing the allocation of medical staff and equipment.\n",
        "\n",
        "3.  **Personalized Treatment Plans:** Understanding a patient's risk profile (derived from the model's predictions and feature importances) can aid in tailoring personalized treatment and management plans, leading to more effective and patient-centric care.\n",
        "\n",
        "4.  **Cost Reduction:** Early detection and effective management can prevent the progression of heart disease, thereby reducing the need for expensive emergency treatments, hospitalizations, and complex surgical procedures in the future. This translates to substantial cost savings for both the healthcare provider and the patients.\n",
        "\n",
        "5.  **Enhanced Patient Engagement:** When patients are presented with a clear risk assessment and the factors contributing to their risk (easily interpretable from a Decision Tree), they are often more motivated to adhere to treatment plans and make necessary lifestyle changes.\n",
        "\n",
        "6.  **Research and Development:** The feature importances derived from the Decision Tree can highlight which clinical factors are most predictive of heart disease. This information can be invaluable for further medical research, drug development, and understanding disease etiology.\n",
        "\n",
        "7.  **Proactive Health Management Programs:** The model can be integrated into population health management systems to identify groups at higher risk across a patient base, enabling the company to launch targeted wellness and prevention programs.\n",
        "\n",
        "In essence, this predictive model acts as a powerful decision-support tool, enhancing diagnostic accuracy, improving patient care efficiency, and contributing to better financial outcomes for the healthcare company while ultimately benefiting patient health."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ac58aa"
      },
      "source": [
        "## Business Value & Deployment\n",
        "\n",
        "### Subtask:\n",
        "Discuss considerations for deployment, interpretability, and ethical implications of the model in a healthcare setting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aee5f1ee"
      },
      "source": [
        "### Deployment Challenges\n",
        "\n",
        "Deploying a machine learning model, even a relatively simple one like a Decision Tree, into a real-world healthcare setting presents several challenges:\n",
        "\n",
        "1.  **Integration with Existing EHR/Clinical Systems:** Healthcare systems often rely on complex, legacy Electronic Health Records (EHR) systems. Integrating a new predictive model requires robust APIs and middleware to ensure seamless data flow (input features for prediction) and output integration (displaying predictions to clinicians). This often involves significant IT infrastructure work, adherence to data standards (e.g., HL7, FHIR), and rigorous testing to prevent disruptions.\n",
        "2.  **Real-time vs. Batch Prediction:** Depending on the use case (e.g., real-time risk assessment during patient admission vs. periodic population-level screening), the deployment architecture must support the required prediction latency. Real-time inference demands highly optimized models and infrastructure.\n",
        "3.  **Scalability and Maintenance:** The model needs to scale with increasing patient data and usage. Furthermore, ongoing maintenance (model retraining, monitoring for data drift or concept drift, software updates) is crucial to ensure the model remains effective and accurate over time. Healthcare data can evolve, requiring adaptive model pipelines.\n",
        "4.  **Security and Data Governance:** Protecting sensitive patient data is paramount. Deployment must comply with strict security protocols, access controls, and data encryption standards to prevent breaches and unauthorized access.\n",
        "\n",
        "### Interpretability\n",
        "\n",
        "Decision Trees possess inherent interpretability, which is a significant advantage in healthcare:\n",
        "\n",
        "1.  **Clinical Trust and Acceptance:** Clinicians are more likely to trust and adopt a model if they can understand *how* it arrives at a particular prediction. A Decision Tree's path from root to leaf node clearly illustrates the sequence of decisions based on specific feature values (e.g., \"If age > 60 AND cholesterol > 240, then disease risk is high\"). This transparency fosters confidence.\n",
        "2.  **Feature Importance Insight:** The tree structure and feature importances (as seen in earlier exercises) directly highlight which patient characteristics or test results are most influential in the prediction. This can validate existing clinical knowledge or even reveal new, subtle relationships that warrant further investigation.\n",
        "3.  **Educational Tool:** Decision trees can serve as an educational tool for medical students or less experienced practitioners, helping them understand complex diagnostic pathways or risk stratification rules derived from data.\n",
        "4.  **Debugging and Bias Detection:** The transparent nature allows for easier identification of potential issues. If a model makes an unexpected or clinically implausible prediction, the decision path can be traced to understand which features led to that outcome, aiding in debugging or uncovering hidden biases.\n",
        "\n",
        "### Ethical Implications\n",
        "\n",
        "Healthcare AI models, especially those for disease prediction, carry profound ethical responsibilities:\n",
        "\n",
        "1.  **Bias and Fairness:**\n",
        "    *   **Source:** Bias can creep in from historical data (e.g., underrepresentation of certain demographic groups, diagnostic practices that historically disadvantaged specific populations). If the training data contains such biases, the model will learn and perpetuate them.\n",
        "    *   **Impact:** A biased model might inaccurately predict disease for certain groups, leading to misdiagnosis, delayed treatment, or unequal access to care. For example, if a model performs poorly on female patients or specific racial groups due to insufficient or biased training data, it can exacerbate existing health disparities.\n",
        "    *   **Mitigation:** Rigorous data auditing, fair representation in training data, bias detection algorithms (e.g., disparate impact analysis), and fairness-aware machine learning techniques are essential. Continuous monitoring of model performance across different demographic subgroups is critical post-deployment.\n",
        "\n",
        "2.  **Data Privacy (HIPAA Compliance):**\n",
        "    *   **Challenge:** Healthcare data is subject to stringent regulations like HIPAA (Health Insurance Portability and Accountability Act) in the US, GDPR in Europe, and similar laws globally. Models must be trained and deployed using de-identified or anonymized data whenever possible.\n",
        "    *   **Mitigation:** Strict data governance, secure data storage, access controls, encryption, and audit trails are non-negotiable. Consent mechanisms for data usage must be robust and transparent.\n",
        "\n",
        "3.  **Accountability and Human Oversight:**\n",
        "    *   **Challenge:** Who is accountable if a model makes a wrong prediction that leads to patient harm? The developer, the deploying institution, or the clinician who acted on the recommendation?\n",
        "    *   **Mitigation:** AI in healthcare should always be viewed as a **decision support tool**, not a decision-maker. Human clinicians must retain ultimate responsibility and oversight. The model's predictions should be presented with confidence scores and explainable rationale (leveraging interpretability) to aid clinical judgment. Clear guidelines for when to override model recommendations are necessary.\n",
        "\n",
        "4.  **False Positives and False Negatives:**\n",
        "    *   **False Positive (Type I Error):** Model predicts disease when there is none. In healthcare, this can lead to unnecessary anxiety, costly follow-up tests (e.g., invasive biopsies), and potentially harmful overtreatment. For example, a false positive heart disease prediction might lead to unwarranted stress tests or even unnecessary medication.\n",
        "    *   **False Negative (Type II Error):** Model predicts no disease when disease is present. This is often more critical in healthcare, as it can result in delayed diagnosis, missed opportunities for early intervention, and progression of serious conditions. A false negative heart disease prediction could mean a patient goes untreated, leading to severe health events.\n",
        "    *   **Mitigation:** The choice of evaluation metrics (precision, recall, F1-score) must be aligned with clinical priorities. In life-threatening conditions, minimizing false negatives (maximizing recall for the positive class) is often prioritized, even at the cost of some false positives. Conversely, for screening tests with severe follow-up procedures, higher precision might be desired. Clinical input is crucial to determine the acceptable trade-off between these errors. Threshold adjustment for classification can also help balance these errors post-training.\n",
        "\n",
        "In conclusion, while Decision Trees offer valuable interpretability for healthcare applications, their deployment requires careful consideration of technical integration, robust ethical safeguards against bias and privacy breaches, and a clear understanding of their role as an assistive tool under human supervision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "574a2939"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Heart Disease dataset, comprising 303 entries and 14 columns, was loaded successfully. Initial exploration revealed 4 missing values in the 'ca' column and 2 in the 'thal' column.\n",
        "*   Missing values in 'ca' and 'thal' were imputed using their respective medians (0.0 for 'ca' and 3.0 for 'thal'), resulting in a complete dataset.\n",
        "*   The original 'target' variable, ranging from 0 to 4, was binarized into 0 (no disease) and 1 (disease present).\n",
        "*   Five categorical features ('cp', 'restecg', 'slope', 'ca', 'thal') were one-hot encoded, expanding the feature set for modeling.\n",
        "*   The dataset was split into training (212 samples) and testing (91 samples) sets using a 70/30 ratio, with stratification to maintain class balance.\n",
        "*   Hyperparameter tuning using GridSearchCV identified optimal parameters for the Decision Tree Classifier: `criterion='entropy'`, `max_depth=5`, `min_samples_leaf=4`, and `min_samples_split=2`.\n",
        "*   The best-tuned Decision Tree model achieved an accuracy of approximately 0.7363 on the test set, with precision, recall, and F1-score also around 0.73-0.74.\n",
        "*   The model offers significant business value in healthcare, including potential for early disease detection, optimized resource allocation, personalized treatment plans, and cost reduction.\n",
        "*   Deployment in healthcare presents challenges such as integration with Electronic Health Records (EHR) systems, ensuring real-time performance, scalability, maintenance, and strict adherence to security and data governance (e.g., HIPAA).\n",
        "*   Decision Trees offer inherent interpretability, which is crucial for gaining clinical trust, understanding feature importance, and serving as an educational tool.\n",
        "*   Ethical considerations are paramount, specifically addressing potential biases in data, ensuring patient data privacy, defining accountability with human oversight, and carefully managing the trade-offs between false positives and false negatives, which have critical implications in clinical settings.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model's current performance (accuracy $\\approx$ 73.63%) suggests it could serve as a valuable decision-support tool, but continuous monitoring and validation with real-world clinical data are essential to confirm its robustness and generalizability.\n",
        "*   Further exploration of feature engineering or more advanced models (e.g., ensemble methods like Random Forest or Gradient Boosting) could potentially improve predictive performance, especially if the current Decision Tree's performance is deemed insufficient for critical clinical applications.\n"
      ]
    }
  ]
}